{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1:Â Using neural nets to recognize handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 ([link](http://neuralnetworksanddeeplearning.com/chap1.html#exercises_191892)): sigmoid neurons simulating perceptrons, part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show that the behavior of a single perceptron doesn't change if we multiply its weight vector and its bias by a constant $c > 0$. The output of this perceptron is:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } cw\\cdot x + cb \\leq 0 \\\\\n",
    "      1 & \\mbox{if } cw\\cdot x + cb > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Which is the same as the output without the multiplication by $c$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Since this is true for every perceptron, the behavior of our neural network as a whole doesn't change either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: sigmoid neurons simulating perceptrons, part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given sigmoid neuron with weight and bias multiplied by $c$, the activation function is $\\sigma(cw.x+cb) = \\frac{1}{1 + e^{-(cw.x+cb)}}$.\n",
    "\n",
    "For each perceptron, it is supposed that $w.x+b \\neq 0$.\n",
    "* If $w.x+b > 0$, then as $c \\rightarrow \\infty$, $\\frac{1}{1 + e^{-(cw.x+cb)}} \\rightarrow \\frac{1}{1 + 0} = 1$.\n",
    "* If $w.x+b < 0$, then as $c \\rightarrow \\infty$, $\\frac{1}{1 + e^{-(cw.x+cb)}} \\rightarrow \\frac{1}{1 + \\infty} = 0$.\n",
    "\n",
    "Therefore our sigmoid neuron gives the same output as a perceptron of parameters $(w,b)$ in the limit as $c \\rightarrow \\infty$.\n",
    "\n",
    "Since this is true for every neuron, our network as a whole has the same behavior as a network of perceptrons as $c \\rightarrow \\infty$.\n",
    "\n",
    "If however we have $w.x+b = 0$ for at least one sigmoid neuron, then its output is going to be $\\sigma(0) = \\frac{1}{1+e^0} = \\frac{1}{2}$ whatever the value of $c$. So we will never have the behavior of a perceptron, which only ever outputs 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The architecture of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple network to classify handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 ([link](http://neuralnetworksanddeeplearning.com/chap1.html#exercise_513527)): converting the output layer into a bitwise representation with an extra layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our extra 4-neuron layer should do:\n",
    "* (digit = 0) (1, 0, 0, 0, 0, 0, 0, 0, 0, 0) -> (0, 0, 0, 0)\n",
    "* (digit = 1) (0, 1, 0, 0, 0, 0, 0, 0, 0, 0) -> (0, 0, 0, 1)\n",
    "* (digit = 2) (0, 0, 1, 0, 0, 0, 0, 0, 0, 0) -> (0, 0, 1, 0)\n",
    "* (digit = 3) (0, 0, 0, 1, 0, 0, 0, 0, 0, 0) -> (0, 0, 1, 1)\n",
    "* (digit = 4) (0, 0, 0, 0, 1, 0, 0, 0, 0, 0) -> (0, 1, 0, 0)\n",
    "* (digit = 5) (0, 0, 0, 0, 0, 1, 0, 0, 0, 0) -> (0, 1, 0, 1)\n",
    "* (digit = 6) (0, 0, 0, 0, 0, 0, 1, 0, 0, 0) -> (0, 1, 1, 0)\n",
    "* (digit = 7) (0, 0, 0, 0, 0, 0, 0, 1, 0, 0) -> (0, 1, 1, 1)\n",
    "* (digit = 8) (0, 0, 0, 0, 0, 0, 0, 0, 1, 0) -> (1, 0, 0, 0)\n",
    "* (digit = 9) (0, 0, 0, 0, 0, 0, 0, 0, 0, 1) -> (1, 0, 0, 1)\n",
    "\n",
    "Our first neuron must output roughly 1 for 8 and 9, and roughly 0 otherwise. The following weight vector will make 8 and 9 the only contributors to its weighted input: (0, 0, 0, 0, 0, 0, 0, 0, 1, 1). With this vector, the weighted input $w.x$ is roughly 1 for digits 8 and 9, and roughly 0 for other digits. More precisely, since the correct output in the old output layer is at least 0.99 and incorrect outputs are smaller than 0.01, $0.99 \\leq w.x \\leq 1.01$ for 8 and 9, and $0 \\leq w.x \\leq 0.02$ for other digits.\n",
    "\n",
    "Now which bias would be appropriate? Let's say we want the same precision as in the old output layer: $0.99 \\leq \\sigma(w.x+b)$ for 8 and 9, and $\\sigma(w.x+b) \\leq 0.01$ for other digits.\n",
    "\n",
    "Or to speak roughly, we look for $b$ satisfying $\\sigma(b) \\approx 0$ and $\\sigma(1+b) \\approx 1$.\n",
    "\n",
    "If we set $b = -0.5$, we're almost there since $\\sigma(b) < 0.5$ and $\\sigma(1+b) > 0.5$. We only have to make the slope much steeper, by multiplying all this by a large constant (meaning a larger weight vector).\n",
    "\n",
    "How large is enough? Remember our desired precision. We are looking for $\\tilde{w}$ such that $0.99 \\leq \\sigma(\\tilde{w}(0.99+b))$ and $\\sigma(\\tilde{w}(0.02+b)) \\leq 0.01$.\n",
    "\n",
    "At this point let's note $\\alpha$ our desired precision: here, $\\alpha = 0.01$.\n",
    "\n",
    "So we want:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      1-\\alpha \\leq \\sigma(\\tilde{w}(1-\\alpha+b)) \\\\\n",
    "      \\sigma(\\tilde{w}(2\\alpha+b)) \\leq \\alpha\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Which gives us:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      \\tilde{w} \\geq \\frac{ln(\\frac{1-\\alpha}{\\alpha})}{1-\\alpha+b} \\\\\n",
    "      \\tilde{w} \\geq -\\frac{ln(\\frac{1-\\alpha}{\\alpha})}{2\\alpha+b}\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      \\tilde{w} \\geq 9.38 \\\\\n",
    "      \\tilde{w} \\geq 9.58\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Before concluding for this neuron, let's generalize to the other ones, because I'd like all the weights and biases of the last layer to be as close to each other as possible (for simplicity).\n",
    "\n",
    "So if we have a neuron that should take the output of $n \\leq 10$ neurons of the old output layer into account (by using a weight vector with only zeros and ones), we'll necessarily have $1-\\alpha \\leq w.x \\leq 1 + (n-1) \\alpha$ when the output is one of those digits, and $0 \\leq w.x \\leq n\\alpha$ for the other digits.\n",
    "\n",
    "So our conditions become:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      1-\\alpha \\leq \\sigma(\\tilde{w}(1-\\alpha+b)) \\\\\n",
    "      \\sigma(\\tilde{w}(n\\alpha+b)) \\leq \\alpha\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "With solution, since $n\\alpha < 0.5$ (and so $n\\alpha+b < 0$):\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      \\tilde{w} \\geq \\frac{ln(\\frac{1-\\alpha}{\\alpha})}{1-\\alpha+b} \\\\\n",
    "      \\tilde{w} \\geq -\\frac{ln(\\frac{1-\\alpha}{\\alpha})}{n\\alpha+b}\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now $n \\leq 10$ for each neuron, so we'll be good everywhere with the following:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      \\tilde{w} \\geq 9.38 \\\\\n",
    "      \\tilde{w} \\geq 11.49\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Or let's say $\\tilde{w} \\geq 12$.\n",
    "\n",
    "So the final weight vector will be made of zeros and twelves, and the final bias will be -6 everywhere.\n",
    "\n",
    "Here's our final solution:\n",
    "\n",
    "* Neuron 1: $w = 12(0, 0, 0, 0, 0, 0, 0, 0, 1, 1)$ ;  $b = -6$\n",
    "* Neuron 2: $w = 12(0, 0, 0, 0, 1, 1, 1, 1, 0, 0)$ ;  $b = -6$\n",
    "* Neuron 3: $w = 12(0, 0, 1, 1, 0, 0, 1, 1, 0, 0)$ ;  $b = -6$\n",
    "* Neuron 4: $w = 12(0, 1, 0, 1, 0, 1, 0, 1, 0, 1)$ ;  $b = -6$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
