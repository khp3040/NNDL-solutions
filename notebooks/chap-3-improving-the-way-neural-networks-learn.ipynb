{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Improving the way neural networks learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross-entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the cross-entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#exercise_35813)): verify that $\\sigma'(z) = \\sigma(z) (1-\\sigma(z))$\n",
    "\n",
    "The definition of the sigmoid function is: $\\sigma(z) = \\frac{1}{1 + e^{-z}} = (1 + e^{-z})^{-1}$.\n",
    "\n",
    "The derivative of the denominator is $-e^{-z}$ and therefore we have $\\sigma'(z) = + e^{-z} (1 + e^{-z})^{-2} = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "And $\\sigma(z)(1 - \\sigma(z)) = \\frac{1}{1 + e^{-z}} \\frac{1 + e^{-z} - 1}{1 + e^{-z}} = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "And so we have $\\sigma'(z) = \\sigma(z) (1-\\sigma(z))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#exercises_824189)): the roles of $y$s and $a$s in the cross-entropy cost function\n",
    "\n",
    "The correct cross-entropy cost function is $-[y \\ln a + (1-y) \\ln (1-a)]$.\n",
    "\n",
    "The (incorrect) similar expression $-[a \\ln y + (1-a) \\ln (1-y)]$ isn't defined when $y = 0$ or $y = 1$, because $\\ln(x)$ isn't defined when $x = 0$.\n",
    "\n",
    "This is an issue because $y = 0$ or $y = 1$ can clearly happen, as $y$ is the correct output (if the expected answer is \"yes\", we would ideally like the network to output exactly 1; in this case we would have $y = 1$).\n",
    "\n",
    "In the right definition, we might think that the same problem would arise when $a = 0$ or $a = 1$. However, this never happens with the sigmoid activation function, because $a = \\sigma(z)$ and whatever the weighted input $z$ for a neuron, we will always have $0 < \\sigma(z) < 1$ by definition of $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: show that the cross-entropy function is still a good cost function when $0 < y < 1$.\n",
    "\n",
    "Namely, we need to show that the cross-entropy cost function $C(a) = - (y \\ln a + (1 - y) \\ln (1 - a))$ is minimized when $a = y$.\n",
    "\n",
    "Let's differentiate $C$:\n",
    "\n",
    "$C'(a) = - \\frac y a + \\frac{1-y}{1-a}$.\n",
    "\n",
    "We look for a local extremum by solving $C'(a) = 0$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        - \\frac y a + \\frac{1-y}{1-a} = 0 &\\iff \\frac y a = \\frac{1-y}{1-a} \\\\\n",
    "        &\\iff y - ay = a - ay \\\\\n",
    "        &\\iff a = y\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We have a unique extremum in $a = y$. To determine whether it is a minimum or a maximum, we compute the second derivative:\n",
    "\n",
    "$C''(a) = \\frac{y}{a^2} + \\frac{1 - y}{(1 - a)^2}$.\n",
    "\n",
    "Since we have supposed $0 < y < 1$, and as always $0 < a < 1$, we have for all $0 < a < 1$: $C''(a) \\geq 0$ (the function is convex). Therefore, the function was minimized when $a = y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#problems_382219)): Many-layer multi-neuron networks\n",
    "\n",
    "For a single training example $x$, we have for the quadratic cost function:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial w_{jk}^L} &= a_k^{L-1} \\delta_j^L \\qquad \\text{(BP4)} \\\\\n",
    "        &= a_k^{L-1} \\frac{\\partial C}{\\partial a_j^L} \\sigma'(z_j^L) \\qquad \\text{(BP1)}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "The cost function for a single training example is $C = \\frac 1 2 \\sum_i (a_i^L - y_i)^2$, so we have $\\frac{\\partial C}{\\partial a_j^L} = a_j^L - y_j$. This gives us:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = a^{L-1}_k  (a^L_j-y_j) \\sigma'(z^L_j)$$\n",
    "\n",
    "And taking all training examples into account,\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = \\frac{1}{n} \\sum\\limits_x a^{L-1}_k  (a^L_j-y_j) \\sigma'(z^L_j)$$\n",
    "\n",
    "Now with the cross-entropy cost function, let's first compute $\\delta^L$ for a single training example $x$: for all neurons $j$ in the $L$th layer,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\delta_j^L &= \\frac{\\partial C}{\\partial a_j^L} \\sigma'(z_j^L) \\qquad \\text{(BP1)} \\\\\n",
    "        &= - \\left( \\frac{y_j}{a_j^L} - \\frac{1 - y_j}{1 - a_j^L} \\right) \\sigma'(z_j^L) \\\\\n",
    "        &= - \\left( \\frac{y_j}{\\sigma(z_j^L)} - \\frac{1 - y_j}{1 - \\sigma(z_j^L)} \\right) \\sigma(z_j^L) \\left( 1 - \\sigma(z_j^L) \\right) \\\\\n",
    "        &= - \\left( y_j (1 - \\sigma(z_j^L)) - (1 - y_j) \\sigma(z_j^L) \\right) \\\\\n",
    "        &= \\sigma(z_j^L) - y_j \\\\\n",
    "        &= a_j^L - y_j\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "And so $\\delta^L = a^L - y$.\n",
    "\n",
    "Let's incorporate it into our previous calculus.\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial w_{jk}^L} &= a_k^{L-1} \\delta_j^L \\qquad \\text{(BP4)} \\\\\n",
    "        &= a_k^{L-1} (a_j^L - y_j)\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "And taking all training examples into account,\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = \\frac{1}{n} \\sum\\limits_x a^{L-1}_k  (a^L_j-y_j)$$\n",
    "\n",
    "For the biases, everything is the same except that instead of using BP4 ($\\frac{\\partial C}{\\partial w_{jk}^L} = a_k^{L-1} \\delta_j^L$), we use BP3 ($\\frac{\\partial C}{\\partial b_j^L} = \\delta_j^L$) and so we don't have the $a_k^{L-1}$ part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: using the quadratic cost when we have linear neurons in the output layer\n",
    "\n",
    "We use the quadratic cost function and the activation function $f: x \\rightarrow x$ in the last layer. We have for a single training example $x$ and for all neurons $j$ in the $L$th layer:\n",
    "\n",
    "$$\\delta_j^L = \\frac{\\partial C}{\\partial a_j^L} f'(z_j^L) \\qquad \\text{(BP1)}$$\n",
    "\n",
    "The cost function for a single training example is $C = \\frac 1 2 \\sum_i (a_i^L - y_i)^2$, so we have $\\frac{\\partial C}{\\partial a_j^L} = a_j^L - y_j$. And $\\forall x \\in \\mathbb{R}, f'(x) = 1$.\n",
    "\n",
    "So $\\delta_j^L = a_j^L - y_j$. In vector form:\n",
    "\n",
    "$$\\delta^L = a^L - y$$\n",
    "\n",
    "Applying BP4 gives us, for a single training example:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial w_{jk}^L} &= a_k^{L-1} \\delta_j^L \\qquad \\text{(BP4)} \\\\\n",
    "        &= a_k^{L-1} (a_j^L - y_j)\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "For the biases we apply BP3:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial b_j^L} &= \\delta_j^L \\qquad \\text{(BP3)} \\\\\n",
    "        &= (a_j^L - y_j)\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "And taking all training examples into account:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = \\frac{1}{n} \\sum_x a^{L-1}_k  (a^L_j-y_j)$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^L_{j}} = \\frac{1}{n} \\sum_x (a^L_j-y_j)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
