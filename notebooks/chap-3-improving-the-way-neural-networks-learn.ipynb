{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Improving the way neural networks learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross-entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the cross-entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#exercise_35813)): verify that $\\sigma'(z) = \\sigma(z) (1-\\sigma(z))$\n",
    "\n",
    "The definition of the sigmoid function is: $\\sigma(z) = \\frac{1}{1 + e^{-z}} = (1 + e^{-z})^{-1}$.\n",
    "\n",
    "The derivative of the denominator is $-e^{-z}$ and therefore we have $\\sigma'(z) = + e^{-z} (1 + e^{-z})^{-2} = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "And $\\sigma(z)(1 - \\sigma(z)) = \\frac{1}{1 + e^{-z}} \\frac{1 + e^{-z} - 1}{1 + e^{-z}} = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "And so we have $\\sigma'(z) = \\sigma(z) (1-\\sigma(z))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#exercises_824189)): the roles of $y$s and $a$s in the cross-entropy cost function\n",
    "\n",
    "The correct cross-entropy cost function is $-[y \\ln a + (1-y) \\ln (1-a)]$.\n",
    "\n",
    "The (incorrect) similar expression $-[a \\ln y + (1-a) \\ln (1-y)]$ isn't defined when $y = 0$ or $y = 1$, because $\\ln(x)$ isn't defined when $x = 0$.\n",
    "\n",
    "This is an issue because $y = 0$ or $y = 1$ can clearly happen, as $y$ is the correct output (if the expected answer is \"yes\", we would ideally like the network to output exactly 1; in this case we would have $y = 1$).\n",
    "\n",
    "In the right definition, we might think that the same problem would arise when $a = 0$ or $a = 1$. However, this never happens with the sigmoid activation function, because whatever the weighted input $z$ for a neuron, we will always have $0 < \\sigma(wz+b) < 1$ by definition of $\\sigma$ (more generally, $\\forall x \\in \\mathbb{R}, 0 < \\sigma(x) < 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: show that the cross-entropy function is still a good cost function when $0 < y < 1$.\n",
    "\n",
    "Namely, we need to show that the cross-entropy cost function $C(a) = - (y \\ln a + (1 - y) \\ln (1 - a))$ is minimized when $a = y$.\n",
    "\n",
    "Let's differentiate $C$:\n",
    "\n",
    "$C'(a) = - \\frac y a + \\frac{1-y}{1-a}$.\n",
    "\n",
    "We look for a local extremum by solving $C'(a) = 0$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        - \\frac y a + \\frac{1-y}{1-a} = 0 &\\iff \\frac y a = \\frac{1-y}{1-a} \\\\\n",
    "        &\\iff y - ay = a - ay \\\\\n",
    "        &\\iff a = y\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We have a unique extremum in $a = y$. To determine whether it is a minimum or a maximum, we compute the second derivative:\n",
    "\n",
    "$C''(a) = \\frac{y}{a^2} + \\frac{1 - y}{(1 - a)^2}$.\n",
    "\n",
    "Since we have supposed $0 < y < 1$, and as always $0 < a < 1$, we have for all $0 < a < 1$: $C''(a) \\geq 0$ (the function is convex). Therefore, the function was minimized when $a = y$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
