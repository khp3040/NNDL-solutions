{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Improving the way neural networks learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross-entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the cross-entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#exercise_35813)): verify that $\\sigma'(z) = \\sigma(z) (1-\\sigma(z))$\n",
    "\n",
    "The definition of the sigmoid function is: $\\sigma(z) = \\frac{1}{1 + e^{-z}} = (1 + e^{-z})^{-1}$.\n",
    "\n",
    "The derivative of the denominator is $-e^{-z}$ and therefore we have $\\sigma'(z) = + e^{-z} (1 + e^{-z})^{-2} = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "And $\\sigma(z)(1 - \\sigma(z)) = \\frac{1}{1 + e^{-z}} \\frac{1 + e^{-z} - 1}{1 + e^{-z}} = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "And so we have $\\sigma'(z) = \\sigma(z) (1-\\sigma(z))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#exercises_824189)): the roles of $y$s and $a$s in the cross-entropy cost function\n",
    "\n",
    "The correct cross-entropy cost function is $-[y \\ln a + (1-y) \\ln (1-a)]$.\n",
    "\n",
    "The (incorrect) similar expression $-[a \\ln y + (1-a) \\ln (1-y)]$ isn't defined when $y = 0$ or $y = 1$, because $\\ln(x)$ isn't defined when $x = 0$.\n",
    "\n",
    "This is an issue because $y = 0$ or $y = 1$ can clearly happen, as $y$ is the correct output (if the expected answer is \"yes\", we would ideally like the network to output exactly 1; in this case we would have $y = 1$).\n",
    "\n",
    "In the right definition, we might think that the same problem would arise when $a = 0$ or $a = 1$. However, this never happens with the sigmoid activation function, because $a = \\sigma(z)$ and whatever the weighted input $z$ for a neuron, we will always have $0 < \\sigma(z) < 1$ by definition of $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: show that the cross-entropy function is still a good cost function when $0 < y < 1$.\n",
    "\n",
    "Namely, we need to show that the cross-entropy cost function $C(a) = - (y \\ln a + (1 - y) \\ln (1 - a))$ is minimized when $a = y$.\n",
    "\n",
    "Let's differentiate $C$:\n",
    "\n",
    "$C'(a) = - \\frac y a + \\frac{1-y}{1-a}$.\n",
    "\n",
    "We look for a local extremum by solving $C'(a) = 0$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        - \\frac y a + \\frac{1-y}{1-a} = 0 &\\iff \\frac y a = \\frac{1-y}{1-a} \\\\\n",
    "        &\\iff y - ay = a - ay \\\\\n",
    "        &\\iff a = y\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We have a unique extremum in $a = y$. To determine whether it is a minimum or a maximum, we compute the second derivative:\n",
    "\n",
    "$C''(a) = \\frac{y}{a^2} + \\frac{1 - y}{(1 - a)^2}$.\n",
    "\n",
    "Since we have supposed $0 < y < 1$, and as always $0 < a < 1$, we have for all $0 < a < 1$: $C''(a) \\geq 0$ (the function is convex). Therefore, the function was minimized when $a = y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#problems_382219)): Many-layer multi-neuron networks\n",
    "\n",
    "For a single training example $x$, we have for the quadratic cost function:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial w_{jk}^L} &= a_k^{L-1} \\delta_j^L \\qquad \\text{(BP4)} \\\\\n",
    "        &= a_k^{L-1} \\frac{\\partial C}{\\partial a_j^L} \\sigma'(z_j^L) \\qquad \\text{(BP1)}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "The cost function for a single training example is $C = \\frac 1 2 \\sum_i (a_i^L - y_i)^2$, so we have $\\frac{\\partial C}{\\partial a_j^L} = a_j^L - y_j$. This gives us:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = a^{L-1}_k  (a^L_j-y_j) \\sigma'(z^L_j)$$\n",
    "\n",
    "And taking all training examples into account,\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = \\frac{1}{n} \\sum\\limits_x a^{L-1}_k  (a^L_j-y_j) \\sigma'(z^L_j)$$\n",
    "\n",
    "Now with the cross-entropy cost function, let's first compute $\\delta^L$ for a single training example $x$: for all neurons $j$ in the $L$th layer,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\delta_j^L &= \\frac{\\partial C}{\\partial a_j^L} \\sigma'(z_j^L) \\qquad \\text{(BP1)} \\\\\n",
    "        &= - \\left( \\frac{y_j}{a_j^L} - \\frac{1 - y_j}{1 - a_j^L} \\right) \\sigma'(z_j^L) \\\\\n",
    "        &= - \\left( \\frac{y_j}{\\sigma(z_j^L)} - \\frac{1 - y_j}{1 - \\sigma(z_j^L)} \\right) \\sigma(z_j^L) \\left( 1 - \\sigma(z_j^L) \\right) \\\\\n",
    "        &= - \\left( y_j (1 - \\sigma(z_j^L)) - (1 - y_j) \\sigma(z_j^L) \\right) \\\\\n",
    "        &= \\sigma(z_j^L) - y_j \\\\\n",
    "        &= a_j^L - y_j\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "And so $\\delta^L = a^L - y$.\n",
    "\n",
    "Let's incorporate it into our previous calculus.\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial w_{jk}^L} &= a_k^{L-1} \\delta_j^L \\qquad \\text{(BP4)} \\\\\n",
    "        &= a_k^{L-1} (a_j^L - y_j)\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "And taking all training examples into account,\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = \\frac{1}{n} \\sum\\limits_x a^{L-1}_k  (a^L_j-y_j)$$\n",
    "\n",
    "For the biases, everything is the same except that instead of using BP4 ($\\frac{\\partial C}{\\partial w_{jk}^L} = a_k^{L-1} \\delta_j^L$), we use BP3 ($\\frac{\\partial C}{\\partial b_j^L} = \\delta_j^L$) and so we don't have the $a_k^{L-1}$ part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: using the quadratic cost when we have linear neurons in the output layer\n",
    "\n",
    "We use the quadratic cost function and the activation function $f: x \\rightarrow x$ in the last layer. We have for a single training example $x$ and for all neurons $j$ in the $L$th layer:\n",
    "\n",
    "$$\\delta_j^L = \\frac{\\partial C}{\\partial a_j^L} f'(z_j^L) \\qquad \\text{(BP1)}$$\n",
    "\n",
    "The cost function for a single training example is $C = \\frac 1 2 \\sum_i (a_i^L - y_i)^2$, so we have $\\frac{\\partial C}{\\partial a_j^L} = a_j^L - y_j$. And $\\forall x \\in \\mathbb{R}, f'(x) = 1$.\n",
    "\n",
    "So $\\delta_j^L = a_j^L - y_j$. In vector form:\n",
    "\n",
    "$$\\delta^L = a^L - y$$\n",
    "\n",
    "Applying BP4 gives us, for a single training example:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial w_{jk}^L} &= a_k^{L-1} \\delta_j^L \\qquad \\text{(BP4)} \\\\\n",
    "        &= a_k^{L-1} (a_j^L - y_j)\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "For the biases we apply BP3:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial b_j^L} &= \\delta_j^L \\qquad \\text{(BP3)} \\\\\n",
    "        &= (a_j^L - y_j)\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "And taking all training examples into account:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = \\frac{1}{n} \\sum_x a^{L-1}_k  (a^L_j-y_j)$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^L_{j}} = \\frac{1}{n} \\sum_x (a^L_j-y_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the cross-entropy to classify MNIST digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the cross-entropy mean? Where does it come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#problem_507295)): why it's not possible to eliminate the $x_j$ term through a clever choice of cost function\n",
    "\n",
    "The derivation of equation (61) began like this, using the chain rule:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_j} = \\frac{\\partial C}{\\partial a} \\frac{\\partial a}{\\partial w_j}$$\n",
    "\n",
    "Since $a = \\sigma(\\sum_i w_i x_i + b)$, we have $\\frac{\\partial a}{\\partial w_j} = x_j \\sigma'(\\sum_i w_i x_i + b) = x_j \\sigma'(z)$.\n",
    "\n",
    "Using the cross-entropy cost function, we have managed to make $\\frac{\\partial C}{\\partial a}$ look like $\\frac{something}{\\sigma'(z)}$, eliminating the $\\sigma'(z)$ term in $\\frac{\\partial C}{\\partial w_j}$.\n",
    "\n",
    "Now we would like to make it look like $\\frac{something}{x_j}$. The problem is that whatever the choice of the cost function $C$, it can only depend on the network output $a$ (and the expected output $y$). Therefore, the contributions of each $x_j$ to the final activation $a$ can't be taken into account by $C$ (keeping in mind that an infinity of choices for the $\\{x_j\\}$ lead to the same weighted input $z$ and the same output $a$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#exercise_332838)): construct an example showing that with a sigmoid output layer, the output activations won't always sum to 1\n",
    "\n",
    "Consider a 2-layer network made of a single input neuron and a single output neuron, with a weight $w$ (a scalar) and bias $b$. Its input (given by the input neuron) is $x$.\n",
    "\n",
    "Now whatever the input $x$, the weighted input will be $z = wx + b$ and the output $a = \\sigma(z) < 1$ since $\\forall x \\in \\mathbb{R}, \\sigma(x) < 1$. So the sum of the output activations, being equal to our unique output activation, won't be 1 (we can also construct examples where the sum of the output activations is more than 1).\n",
    "\n",
    "By contrast, had we used the softmax output layer in this case, we would have had:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\sum_j a_j &= \\frac{\\sum_j e^{z_j}}{\\sum_k e^{z_k}} \\\\\n",
    "        &= \\frac{e^z}{e^z} \\\\\n",
    "        &= 1\n",
    "    \\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#exercises_193619)): monotonicity of softmax\n",
    "\n",
    "For easier differentiation, let's reformulate $a_j^L$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        a_j^L &= \\frac{e^{z_j^L}}{e^{z_j^L} + \\sum\\limits_{k \\neq j} e^{z_k^L}} \\\\\n",
    "        &= \\frac{1}{1 + e^{-z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L}} \\\\\n",
    "        &= \\left( 1 + e^{-z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right) ^{-1}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Now\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial a_j^L}{\\partial z_j^L} &= - \\frac{\\partial \\left( 1 + e^{-z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right)}{\\partial z_j^L} \\left( 1 + e^{-z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right) ^{-2} \\\\\n",
    "        &= \\left( e^{z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right) \\left( 1 + e^{-z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right) ^{-2} \\\\\n",
    "        &= \\left( e^{z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right) \\left( a_j^L \\right) ^2 \\\\\n",
    "        &> 0\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "And for $k \\neq j$,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial a_j^L}{\\partial z_k^L} &= - \\frac{\\partial \\left( 1 + e^{-z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right)}{\\partial z_k^L} \\left( 1 + e^{-z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right) ^{-2} \\\\\n",
    "        &= - \\left( e^{-z_j^L} e^{z_k^L} \\right) \\left( 1 + e^{-z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right) ^{-2} \\\\\n",
    "        &= - \\left( e^{-z_j^L} e^{z_k^L} \\right) \\left( a_j^L \\right) ^2 \\\\\n",
    "        &< 0\n",
    "    \\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: non-locality of softmax\n",
    "\n",
    "We just showed that for $k \\neq j$, $\\frac{\\partial a_j^L}{\\partial z_k^L} \\neq 0$. This shows that $a_j^L$ depends on all weighted inputs $z_k^L$, not just $z_j^L$.\n",
    "\n",
    "But the previous derivation wasn't actually necessary to see that. Recall that with a softmax output layer, $a_j^L = \\frac{e^{z_j^L}}{\\sum_k e^{z_k^L}}$. Because of the sum in the denominator, we see directly that changing the value of any $z_k^L$ will change the value of $a_j^L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#problem_905066)): Inverting the softmax layer\n",
    "\n",
    "By definition:\n",
    "\n",
    "$$a_j^L = \\frac{e^{z_j^L}}{\\sum_k e^{z_k^L}}$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$z_j^L = \\ln \\left( a_j^L \\right) + \\ln \\left( \\sum_k e^{z_k^L} \\right)$$\n",
    "\n",
    "We just have to call $C$ the constant $\\ln \\left( \\sum_k e^{z_k^L} \\right)$, which is independent of $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 ([link](http://neuralnetworksanddeeplearning.com/chap3.html#problems_919607)): derive equations (81) and (82)\n",
    "\n",
    "Let's first derive equation (81): $\\frac{\\partial C}{\\partial b_j^L} = a_j^L - y_j$.\n",
    "\n",
    "For clarity, let's call $y$ the expected output vector, made only of $0$s and $1$s, and $\\tilde{y}$ the integer such that $y_\\tilde{y} = 1$ (in the MNIST example, the correct digit).\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial b_j^L} &= \\delta_j^L \\qquad \\text{(BP3)} \\\\\n",
    "        &= \\frac{\\partial C}{\\partial z_j^L} \\qquad \\text{by definition of } \\delta_j^L \\\\\n",
    "        &= \\sum\\limits_k \\frac{\\partial C}{\\partial a_k^L} \\frac{\\partial a_k^L}{\\partial z_j^L} \\qquad \\text{(chain rule)} \\\\\n",
    "        &= \\frac{\\partial C}{\\partial a_\\tilde{y}^L} \\frac{\\partial a_\\tilde{y}^L}{\\partial z_j^L} \\qquad \\text{as } C \\text{ only depends on } a_\\tilde{y}^L \\\\\n",
    "        &= - \\frac{1}{a_\\tilde{y}^L} \\frac{\\partial a_\\tilde{y}^L}{\\partial z_j^L} \\qquad \\text{as } C = - \\ln a_\\tilde{y}^L\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "At this point, we must treat 2 cases separately, using the expressions derived in Exercise 5.\n",
    "\n",
    "* If $j = \\tilde{y}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial b_j^L} &= - \\frac{1}{a_j^L} \\left( e^{z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right) \\left( a_j^L \\right) ^2 \\qquad \\text{using the first expression from Exercise 5} \\\\\n",
    "        &= - a_j^L \\left( 1 +  e^{z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} - 1 \\right) \\\\\n",
    "        &= - a_j^L \\left( \\frac{1}{a_j^L} - 1 \\right) \\qquad \\text{recalling from Exercise 5 that } a_j^L = \\left( 1 + e^{-z_j^L} \\sum\\limits_{k \\neq j} e^{z_k^L} \\right) ^{-1} \\\\\n",
    "        &= a_j^L - 1 \\\\\n",
    "        &= a_j^L - y_j \\qquad \\text{since } y_j = y_\\tilde{y} = 1\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "* If $j \\neq \\tilde{y}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\frac{\\partial C}{\\partial b_j^L} &= - \\frac{1}{a_\\tilde{y}^L} \\left(- e^{-z_\\tilde{y}^L} e^{z_j^L} \\right) \\left( a_\\tilde{y}^L \\right) ^2 \\qquad \\text{using the second expression from Exercise 5} \\\\\n",
    "        &= a_\\tilde{y}^L e^{-z_\\tilde{y}^L} e^{z_j^L} \\\\\n",
    "        &= \\frac{e^{z_\\tilde{y}^L}}{\\sum_k e^{z_k^L}} e^{-z_\\tilde{y}^L} e^{z_j^L} \\qquad \\text{by definition of } a_\\tilde{y}^L \\\\\n",
    "        &= \\frac{e^{z_j^L}}{\\sum_k e^{z_k^L}} \\\\\n",
    "        &= a_j^L \\\\\n",
    "        &= a_j^L - y_j \\qquad \\text{since } j \\neq \\tilde{y} \\text{ and so } y_j = 0\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We have proven Equation (81): $\\frac{\\partial C}{\\partial b_j^L} = a_j^L - y_j$.\n",
    "\n",
    "The proof for equation (82) is exactly the same, except that instead of starting with $\\frac{\\partial C}{\\partial b_j^L} = \\delta_j^L$ using BP3, it starts with $\\frac{\\partial C}{\\partial w_{jk}^L} = a_k^{L-1} \\delta_j^L$ using BP4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: explanation of the \"softmax\" name\n",
    "\n",
    "Let's consider $a^L_j = \\frac{e^{c z^L_j}}{\\sum_k e^{c z^L_k}}$ with $c > 0$.\n",
    "\n",
    "* We still have $a_j^L \\geq 0$ for all $j$\n",
    "* The sum of the outputs of all neurons is still 1: $\\sum\\limits_j a_j^L = \\frac{\\sum\\limits_j e^{c z^L_j}}{\\sum\\limits_k e^{c z^L_k}} = 1$.\n",
    "\n",
    "Therefore, the output activations still form a probability distribution.\n",
    "\n",
    "What about the limit as $c \\rightarrow + \\infty$?\n",
    "\n",
    "Let's write the expression slightly differently:\n",
    "\n",
    "$$a_j^L = \\frac{1}{1 + e^{-c z_j^L} \\sum\\limits_{k \\neq j} e^{c z_k^L}} = \\frac{1}{1 + \\sum\\limits_{k \\neq j} e^{c( z_k^L - z_j^L)}}$$\n",
    "\n",
    "* If $z_j^L$ is not the maximum weighted input (there exists $m$ such that $z_m^L > z_j^L$), then we will have:\n",
    "\n",
    "$$\\lim_{c \\to + \\infty} e^{c(z_m^L - z_j^L)} = + \\infty $$\n",
    "\n",
    "And since all other terms in the sum are positive:\n",
    "\n",
    "$$\\lim_{c \\to + \\infty} \\sum\\limits_{k \\neq j} e^{c(z_k^L - z_j^L)} = + \\infty $$\n",
    "\n",
    "And therefore:\n",
    "\n",
    "$$\\lim_{c \\to + \\infty} a_j^L = 0$$\n",
    "\n",
    "* If $z_j^L$ is one of the $n \\geq 1$ maximal weighted inputs, we will have:\n",
    "  * for $k$ such that $z_k^L$ is another maximal weighted input, $\\lim_{c \\to + \\infty} e^{c(z_k^L - z_j^L)} = 1$ since $z_k^L - z_j^L = 0$;\n",
    "  * for $k$ such that $z_k^L$ is not one of the maximal weighted inputs, $\\lim_{c \\to + \\infty} e^{c(z_k^L - z_j^L)} = 0$ since $z_k^L - z_j^L < 0$;\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\\lim_{c \\to + \\infty} \\sum\\limits_{k \\neq j} e^{c(z_k^L - z_j^L)} = n - 1$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\\lim_{c \\to + \\infty} a_j^L = \\frac 1 n$$\n",
    "\n",
    "(in particular, $\\lim_{c \\to + \\infty} a_j^L = 1$ if $z_j^L$ is the unique maximum).\n",
    "\n",
    "More succinctly,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "   \\lim_{c \\to + \\infty} a_j^L = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } z_j^L \\mbox{ is not a maximum weighted input} \\\\\n",
    "      \\frac 1 n & \\mbox{if } z_j^L \\mbox{ is one of } n \\mbox{ maximal weighted inputs}\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now we see that when $c = 1$, we still put more weight on the bigger values because of the exponential function, but we take all of them into account, not just the maximal ones, hence the \"softmax\" name."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
